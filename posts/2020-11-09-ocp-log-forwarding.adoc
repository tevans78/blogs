---
layout: post
title: "Forwarding Open Liberty logs in OpenShift Container Platform to Splunk using Log Forwarding API"
categories: blog
author_picture: https://avatars3.githubusercontent.com/melissa-lee
author_github: https://github.com/melissa-lee
seo-title: Forwarding Open Liberty logs in OpenShift to Splunk by using the Log Forwarding API - OpenLiberty.io
seo-description: Using the Log Forwarding API, you can send logs from Open Liberty that are deployed in OpenShift Container Platform to remote destinations. You can deploy Splunk on your external machine or inside your OpenShift Container Platform. The logs can be forwarded to Splunk using the Fluentd forward protocol.
blog_description: Using the Log Forwarding API, you can send logs from Open Liberty deployed in OpenShift Container Platform to remote destinations. You can deploy Splunk on your external machine or inside your OpenShift Container Platform. The logs can be forwarded to Splunk using the Fluentd forward protocol.
additional_authors:
- name: David Chan
  github: https://github.com/Channyboy
  image: https://avatars0.githubusercontent.com/Channyboy
---
= Forwarding Open Liberty logs in OpenShift to Splunk by using the Log Forwarding API
Halim Lee <https://github.com/leemelim>

Collecting logs in a single location helps you monitor all of your applications efficiently, which is especially important in highly distributed and dynamic container orchestration systems. Red Hat OpenShift Container Platform (OpenShift) provides a built-in log aggregation solution that uses Elasticsearch, Fluentd, and Kibana, also known as the EFK stack. In addition to the EFK stack, you can use the https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-external.html#cluster-logging-collector-log-forwarding-about_cluster-logging-external[Log Forwarding API] to send logs from an OpenShift cluster to other log analysis solutions. The Log Forwarding API is available starting with OpenShift 4.6, and in OpenShift 4.3 to 4.5 as a Technology Preview feature.

Splunk is one of the most popular log analysis solutions. If you want to start using Splunk as your logging solution for Open Liberty on OpenShift, you must first integrate OpenShift with Splunk. The Log Forwarding API simplifies OpenShift cluster integration with Splunk. You can configure custom pipelines to send logs to endpoints that are either inside or outside of your OpenShift cluster.

To send logs from OpenShift to Splunk, the Log Forwarding API must first send them to a Fluentd server. This blog post walks you through configuring the Log Forwarding API on your OpenShift cluster to forward your logs to a Fluentd and Splunk deployment that is external to your cluster.  We also cover the configuration and deployment of the Fluentd and Splunk server on an external machine. The Fluentd server redirects the forwarded logs to Splunk by using Splunk's HTTP Event Collector (HEC) API. In the final sections, you integrate Open Liberty logs with the Fluentd forwarder and setup Splunk dashboards that are provided by the Open Liberty team.

If you would rather set up the Log Forwarding API, Fluentd forwarder, and Splunk locally within your OpenShift cluster, see link:https://www.openshift.com/blog/forwarding-logs-to-splunk-using-the-openshift-log-forwarding-api[Forwarding Logs to Splunk Using the OpenShift Log Forwarding API] on the OpenShift blog. After you complete those steps, proceed to the <<integrating-open-liberty-json-logs,Integrating Open Liberty JSON Logs>> and <<setting-up-splunk-dashboard,Setting Up Splunk-Dashboard>> sections of this blog post to complete your setup.


== Before you begin

. Ensure that your cluster logging instance is created and all pods are fully operational. For more information, see the link:https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-deploying.html[Deploying cluster logging] documentation for your OpenShift version.
+
*Note:* The cluster logging instance sample that is provided in the OpenShift documentation leaves the _storage class_ value for the reader to configure. In test environments, you can omit the storage section and the cluster logging instance uses in-memory storage. However, this configuration is not suitable for production.

. [[keycert-secret]]*Optional:* It is recommended that you secure the connection between the Fluentd servers on your OpenShift cluster and the external Fluentd server. This step creates a _secret_ that is used by the Log Forwarding API to achieve a secure connection. This procedure assumes that the necessary key and certificate files, for example `tls.crt` and `tls.key`, are already created. The instructions for creating the keys and certificates are not within the scope of this blog post. The key and certificate are also used when you <<keycert-fluentd, deploy the external Fluentd server>>.
+
--
.. Switch to the `openshift-logging` project
+
[source]
----
oc project openshift-logging
----
.. Create a secret that contains your key and certificate. You also need to specify a _shared_key_, which is a string password that is used as an extra layer of authentication between the Fluentd servers on the OpenShift cluster and the external Fluentd server. The examples in this blog post use _secure-forward_ for the secret name and  _"secretpassword"_ for the shared key. Change the paths in the following example as necessary:
+
[source]
----
oc create secret generic secure-forward --from-file=ca-bundle.crt=/path/to/tls.crt --from-file=tls.crt=/path/to/tls.crt --from-file=tls.key=/path/to/tls.key   --from-literal=shared_key=secretpassword
----
--

== Configuring the Log Forwarding API

. Create the `log-forward-instance.yaml` file. Configure the outputs and pipelines.
+
--
* The `outputs` section of the file specifies where to send the logs. To establish a secure connection to an output, you must specify a secret, such as the one that you created in <<keycert-secret,step 2b of the *Before you begin* section>>. If you want to use an insecure connection for test purposes, you can omit the secret from your output configuration.
* The `pipelines` section of the file specifies which log types are sent to the configured outputs.
+
The following sample `log-forward-instance.yaml` file defines two `outputs`: a secure connections to a Fluentd server named `fluentd-server-secure` and an insecure connection to another Fluentd server named `fluentd-server-insecure`. Note the different protocols that are defined for both in the _url_ (tls vs tcp) . Remove the output type that you do not wish to use in your file. Each log type and its corresponding output reference is defined in the `pipelines` section. The `default` output reference points to the internal OpenShift Container Platform Elasticsearch instance:
+
```
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging
spec:
  disableDefaultForwarding: true
  outputs:
   - name: fluentd-server-secure
     type: fluentdForward
     url: 'tls://fluntdserver.security.example.com:24224' # Substitute with FQDN or IP address
     secret:
        name: secure-forward
   - name: fluentd-server-insecure
     type: fluentdForward
     url: 'tcp://fluntdserver.home.example.com:24224' # Substitute with FQDN or IP address
  pipelines:
   - name: container-logs
     inputRefs:
     - application
     outputRefs:
     - default
     - fluentd-server-secure
   - name: infra-logs
     inputRefs:
     - infrastructure
     outputRefs:
     - default
   - name: audit-logs
     inputRefs:
     - audit
     outputRefs:
     - default

```
+
For more information, see link:https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-external.html#cluster-logging-collector-log-forward-fluentd_cluster-logging-external[Forwarding logs using the Fluentd forward protocol]. Make sure to consult the documentation for your version of OpenShift, as the pipeline configuration for version 4.6 is slightly different from the preceding versions.
--
. Create the Log Forwarding instance inside your OpenShift cluster:
+
[source]
----
[root@ocp ~]# oc create -f log-forward-instance.yaml
----
+

. **OpenShift 4.3 to 4.5 only:** Annotate the ClusterLogging instance to enable the Log Forwarding API.
+
[source]
----
[root@ocp ~]# oc annotate clusterlogging -n openshift-logging instance clusterlogging.openshift.io/logforwardingtechpreview=enabled
----
+


. To check whether the logs are forwarded to the specified outputs, run the following command:
+
[source]
----
[root@ocp ~]# oc -n openshift-logging get cm fluentd -o json | jq -r '.data."fluent.conf"' > fluentd-with-logfowarding.conf
----
+
This command displays the ConfigMap configuration for Fluentd inside OpenShift. Check whether the outputs are defined inside the configuration file.

* In the following configuration file example, you can see the `elasticsearch` and `fluent-forward` outputs referenced by `@label`:
+
```
...
<label @CONTAINER_LOGS>
  <match **>
    @type copy

    <store>
      @type relabel
      @label @ELASTICSEARCH
    </store>
    <store>
      @type relabel
      @label @FLUENTD_FORWARD
    </store>
  </match>
</label>
...
```
+


== Configuring Splunk and Fluentd

When you specify the`forward` output in your `log-forward-instance.yaml` file, you can forward OpenShift logs to Splunk by using the Fluentd forward protocol. You can set up Splunk inside your OpenShift Cluster or on your external machine.

=== Setting up Splunk and Fluentd on your external machine

The following instructions explain how to manually set up Splunk and Fluentd on your external machine. If you already have Splunk deployed on your external machine, this option helps you set up the connection between your OpenShift cluster and Splunk. To receive logs from Fluentd inside your OpenShift cluster, you must deploy both Splunk and an instance of Fluentd on your machine. For the  purposes of this setup demo, Docker compose is used for installation and the external deployment of Fluentd and Splunk.


. Create the following directories to hold the necessary files:
+
[source]
----
/path/to/fluentdSplunkDir
/path/to/fluentdSplunkDir/fluentd
/path/to/fluentdSPlunkDir/fluentd/conf
/path/to/fluentdSPlunkDir/fluentd/secret
----

. Create a `Dockerfile` file under the `/path/to/fluentdSplunkDir/fluentd` directory to install essential packages while you build the Fluentd Docker image. You need to install the *build-essential* package to install all dependencies and the *fluent-plugin-splunk-enterprise* package to forward the logs to Splunk.
* Sample `Dockerfile`:
+
```
# fluentd/Dockerfile
FROM fluent/fluentd:v1.10-debian
user 0
RUN apt-get update -y
RUN apt-get install build-essential -y
RUN fluent-gem install fluent-plugin-splunk-enterprise -v 0.10.0
```
+

. [[keycert-fluentd]]*Optional:* If you are configuring a secure connection between your external Fluentd server and the Fluentd servers from your OpenShift cluster, move the  <<keycert-secret,`tls.key` and `tls.crt` files that you created earlier>> to the `/path/to/fluentdSplunkDir/secret` directory.

. Create the `docker-compose.yaml` file under the `/path/to/fluentdSplunkDir` directory for Fluentd and Splunk deployment on your external machine.
+
--
* Sample `docker-compose.yaml`:
```
version: '3'

services:
  splunk:
    hostname: splunk
    image: splunk/splunk:latest
    environment:
      SPLUNK_START_ARGS: --accept-license
      SPLUNK_ENABLE_LISTEN: 8088
      SPLUNK_PASSWORD: changeme
    ports:
      - "8000:8000"
      - "8088:8088"

  fluentd:
    build: ./fluentd
    volumes:
      - ./fluentd/conf:/fluentd/etc
      - ./fluentd/secret:/fluentd/secret # remove if not using a secure connection
    links:
      - "splunk"
    ports:
      - "24224:24224"
      - "24224:24224/udp"
```
Configure the ports for Splunk and Fluentd. You can also define a splunk password under *splunk: environment*.

--

. Create `fluent.conf` file in the `/path/to/fluentdSplunkDir/fluentd/conf/` directory to configure Fluentd.
+
--
The following `fluent.conf` file uses a *secure* connection between OpenShift Fluentd servers:
```
<source>
  @type forward
  port 24224
  <transport tls>
    cert_path /fluentd/secret/tls.crt
    private_key_path /fluentd/secret/tls.key
  </transport>
  <security>
    self_hostname fluentd
    shared_key secretpassword
  </security>
</source>

<match kubernetes.**>
  @type splunk_hec
  host splunk
  port 8088
  token 00000000-0000-0000-0000-000000000000 # substitute with token

  default_source openshift

  use_ssl true
  ssl_verify false  # skips SSL certificate verification
  #ca_file /path/to/ca.pem

  flush_interval 5s
</match>

```

* The *source* directive determines the input sources. It uses the *forward* type to accept TCP packets from your OpenShift instance.
** *port* indicates the port that the Fluentd server is listening to for data
** The *transport* section with the *tls*  parameter enables a secure TLS connection between this Fluentd server and fluentd servers in the OpenShift cluster.
*** The  *cert_path* and *private_key_path* parameters are the keys and certificates that are mounted into the Fluentd docker image.
** The *security* section is used for additional authentication
*** The *self_hostname* parameter is a required key that indicates the name of the host. This sample uses _fluentd_.
*** The *shared_key* parameter connects the Fluentd servers by using password authentication. This example uses uses _secretpassword_ as the password.
**** If you choose to use an *insecure* connection between the Fluentd servers in the OpenShift cluster and this Fluentd server, you can use the following simplified source configuration instead:
+
```
<source>
  @type forward
  port 24224
</source>
```

* The *match* directive determines the output destinations. It looks for events with matching tags and uses *splunk_hec* to send the events to Splunk by using HTTP Event Collector.
** The Splunk *host* value is required. We are using  _"splunk"_ for the host, as defined in the `docker-compose.yml`.
** The Splunk *port* value is required. We are using port `8088`, as defined in the `docker-compose.yml`.
** Replace [[fluent-conf]]*token* with the Splunk generated token. This token is obtained later in <<splunk-token,step 7>>.
** The *default_source* parameter sets the value as source metadata.
** Set the *use_ssl* parameter to true to use SSL when you connect to Splunk. By default, the Splunk deployment enables SSL for incoming HEC connections.
** The *ssl_verify* parameter is set to false to avoid SSL certificate verification. Since both the Fluentd and Splunk images are deployed on the same machine, this blog post uses an insecure connection. To secure your connection with Splunk, configure a certificate for your splunk deployment, load it into your Fluentd image, and point to it with the *ca_file* option. These steps are beyond the scope of this blog post.

See the link:https://docs.fluentd.org/input/forward[Fluentd documentation for the _forward_ input plugin] for more configuration options.

The Fluentd image that is used in this blog post has Fluent's Splunk HEC output plugin installed. See the link:https://github.com/fluent/fluent-plugin-splunk/blob/2247356927cab421af1ddb7d22bd8046726c8d62/README.hec.md[Splunk HTTP Event Collector Output Plugin documentation] for more configuration options.
--

. Deploy Splunk by running the following command:
+
[source]
----
[root@ocp ~]# docker-compose up splunk
----
+


. [[splunk-token]]Create the Splunk HTTP Event Collector data input token. Visit Splunk at `http://localhost:8000` and log in with `admin` and the password that is specified in your `docker-compose.yaml` file. Go to *Settings* > *Data Inputs* > *HTTP Event Collector* > *New Token*. Set `Name` as "openshift" and hit next. In Input Settings, set `Source Type` as "Automatic" and `App Context` as "Search & Reporting (search)". Under `Index`, click `Create a new index` and set `Index Name` as "openshift".
+
image::/img/blog/splunk-index.png[Splunk Index,width=70%,align="center"]
+
Select the "openshift" index in the `Available item(s)` box to move the index into the `Selected itm(s)` box.
+
image::/img/blog/splunk-openshift-index.png[Splunk Openshift Index,width=70%,align="center"]
+
Proceed to the `Review` section and submit. Copy the generated token value to use in the <<fluent-conf, fluent.conf file>>.
+
. Deploy Fluentd by running the following command:
+
[source]
----
[root@ocp ~]# docker-compose up fluentd
----
+


== Integrating Open Liberty JSON Logs

Open Liberty application pods output logs in JSON format. To enable Fluentd to parse the JSON fields from the message body, first change the cluster logging instance's managementState field from "Managed" to "Unmanaged":

```
[root@ocp ~]# oc edit ClusterLogging instance

apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"

....

spec:
  managementState: "Unmanaged"
```

Alternatively, you can modify this file on the web console. Ensure you are in the `openshift-logging` project namespace and go to `Administration > Custom Resource Definitions > ClusterLogging > Instances > Instance > YAML`. Change the `managementState` field from "Managed" to "Unmanaged".

Next, we'll need to change the  *merge_json_log* attribute of the `kubernetes.var.log.containers.**` filter to *true* in the Fluentd's config map. If you are running on Open Shift 4.3 to 4.5 this step will be different. See below for the appropriate steps.

```
[root@ocp ~]# oc edit configmap fluentd

apiVersion: v1
data:
...
  <filter kubernetes.var.log.containers.**>
    @type parse_json_field
    merge_json_log 'true'
    preserve_json_log 'true'
    json_fields 'log,MESSAGE'
  </filter>
...
```

Alternatively, you can modify this file on the web console. Ensure you are in the `openshift-logging` project namespace and go to `Workloads > Config Maps > fluentd > YAML`.  Change the value for the `merge_json_log` attribute of the `kubernetes.var.log.containers.**` filter to true.

**OpenShift 4.3 to 4.5 only:** Instead of manually changing the config map you can run:
[source]
----
[root@ocp ~]# oc set env ds/fluentd MERGE_JSON_LOG=true
----

Finally we need to force restart the Fluentd pods to pick up this new change:
[source]
----
oc delete pod --selector logging-infra=fluentd
----

*Note:* After you set the `managementState` to *"Unmanaged"* in the cluster logging instance, any further changes to the _ClusterLogging_ or _ClusterLogForwarder_ instances are not automatically detected. If you need to fix or modify the configuration for either you will need to change the *managementState* field back to *"Managed"*. However, once the `managementState` is changed to *"Managed"* the cluster logging instance will revert the Fluentd's config map back to the original settings.

== Setting up the Splunk dashboard and viewing logs

. Go to Search & Reporting. Search for `index="openshift"` to view logs from OpenShift Container Platform.

. Download the link:https://github.com/WASdev/sample.dashboards/tree/2ef92498e507657e1e718659184f46ff4826d2ce/Liberty/OCP/Splunk%208[Sample dashboard for Liberty inside OpenShift Container Platform using Splunk 8].

. Under the Search & Reporting view, go to the _Dashboards_ tab, click `Create New Dashboard`, and give it a name, for example, `Liberty Problems Dashboard`.

. Import the downloaded sample dashboards by using the *Source* option. Using this dashboard, you can visualize message, trace, and first failure data capture (FFDC) logging data that is collected from JSON logging in Open Liberty.

image::/img/blog/splunk-dashboard.png[Splunk-Dashboard,width=70%,align="center"]

== Troubleshooting

If you find that there are no logs present on Splunk when you are done configuring, there are a few approaches to diagnose the issue.

*Connection between Fluentd and Splunk*

* Ensure that the Splunk HEC token is correct
* Check the container logs from the Fluentd instance and the Splunk instance for warnings or errors

*Connection between the OpenShift cluster and the Fluentd instance*

* Ensure that the IP/FQDN of the machine that is hosting Fluentd and Splunk is accessible from the OpenShift cluster.
* (Security) Ensure that you are using the correct key and certificates for both the OpenShift _secret_ and the Fluentd instance.
* (Security) Ensure that you are using the correct `shared_key` value for both the OpenShift _secret_ and the Fluentd instance.
* Check the logs for the Fluentd pods that are running under the `openshift-logging` namespace for warnings or errors.


== Conclusion
Application logging helps you easily retrieve and analyze problems on your servers. With the Log Forwarding API, you can use existing external enterprise log collection solutions for OpenShift logs. This post demonstrates how Splunk can help you to aggregate and analyze log events from Open Liberty servers that are running on OpenShift.
